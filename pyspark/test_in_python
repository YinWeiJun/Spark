1. python测试pyspark
    from pyspark.sql import SparkSession
    from pyspark import SparkContext
    from pyspark.sql import Row
    from pyspark.sql import SQLContext,HiveContext
    from pyspark.sql.functions import udf
    from pyspark.sql.types import *
    
    spark = SparkSession.builder.master("local").appName("test").config("k","v").getOrCreate()
    
    # 创建DataFrame
    ll = [(1, {"a":1, "b":2}),(2, {"c":3, "d":4})]
    df = spark.createDataFrame(ll, ['id', 'value'])
    df.collect()
    >>>[Row(id=1, value={u'a': 1, u'b': 2}), Row(id=2, value={u'c': 3, u'd': 4})]
    
    # 在DataFrame中查询数据
    df.createOrReplaceTempView("tb")
    ids_df = spark.sql('select id from tb')
    ids_pd = ids_df.toPandas()
    ids_pd['id'][0]
    >>>1 (<type 'numpy.int64'>)
    # 获取具体的值
    id = ids_pd['id'][0].item()
    id
    >>>1 (<type 'int'>)
    
    # 解析DataFrame的数据，并添加一列信息
    def parse(id):
        return '1'
    parse_udf = udf(parse, StringType())
    df2 = df.withColumn("test", parse_udf(df["id"]))
    df2.collect()
    >>>[Row(id=1, value={u'a': 1, u'b': 2}, test=u'1'), Row(id=2, value={u'c': 3, u'd': 4}, test=u'1')]
    
    # 反解析数据
    def load_v(v):
        d = json.loads(v)
        return [d["a"]]
    res = ddff.rdd.flatMap(lambda x: load_v(x["test"]))
    res.first()
    >>>1
    
a.empty    if(a.empty):print("!!")    判断a是否为空

a.item()     没有用过，应该a.item(i)  表示第i个节点

a.any()       if(a.any() in [1,2,3,4]):print("!!")    判断 a中的任意一个值是否在[1,2,3,4]中

a.all()         if(a.all() in [1,2,3,4]):print("!!")    判断 a中的所有值是否在[1,2,3,4]中

    
