1. spark调用之前，需要先申请通道，spark-submit,如：
    park-submit \
        --driver-memory 4G\
        --num-executors 100 \
        --executor-memory 6G \
        --executor-cores 4 \
        --conf spark.default.parallelism=1000 \
        --conf spark.storage.memoryFraction=0.5 \
        --conf spark.shuffle.memoryFraction=0.3 \
        --conf spark.yarn.executor.memoryOverhead=8192 \
        --conf spark.sql.catalogImplementation=hive \
        --queue root.dx_main_prod \
        GetTrac.py

2. DataFrame转rdd
    dataFrame.rdd
  
3. 常用数据转换模式：将数据信息，进行处理存到心得column里面，然后获取就可以了，如：
    parse_udf = udf(self.parse_coordinates, StringType()) //self.parse_coordinate接受datas参数，返回字符串
    datas_new = datas.dropna() //删除没有值的列
    datas_new = datas_new.withColumn('trac_info', parse_udf(datas)) //新增一列
    datas_new = datas_new.fillna('none') //没有值的地方填充‘none’
    
4. 去除重复行
    df = df.dropDuplicates()
    
5. 添加ID列
    from pyspark.sql import functions
    df.withColumn('new_id', functions.monotonically_increasing_id()).show()
    
6. spark为分布式程序，调用函数不能为类内部的函数，必须是全局函数

7. spark文件存储路径只能为申请的用户的路径下

8. spark会对每个用户所存的文件数量进行限制，但是一般不会对文件大小进行限制。原因在于文件大小影响是存储，如果不够可以多加几台机器，不是大问题；
但是每个用户存储的文件，并不是都在一台机器上，会分布到多台机器上，spark需要一台调度机器存储这个分布位置，相当于存一个索引文件，如果每个用户文件
过度，索引文件就会过大，所以需要限制。

9. 拉取文件： hadoop fs -get/-ls等等

10. spark上的程序想要统计某个事件发生的次数，可以用累加器 accm = sc.accumulator, 在事件中进行全局声明global accm,然后调用accm += 1,
最后通过accm.value输出即可
